{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a84e57c3-4ec9-4a60-9982-44078370f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Vulnerability Detection Script\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "\n",
    "# Load non-vulnerable whitelist terms\n",
    "with open(\"nvw.txt\", \"r\") as f:\n",
    "    non_vulnerable_whitelist = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "# ---------------------- Part 1: Load & Prepare Data ----------------------\n",
    "\n",
    "def load_training_data():\n",
    "    with open(\"improved_positive_training_data.txt\", \"r\") as pos_file:\n",
    "        positive_sentences = [line.strip() for line in pos_file if line.strip()]\n",
    "    with open(\"negative_training_data (2).txt\", \"r\") as neg_file:\n",
    "        negative_sentences = [line.strip() for line in neg_file if line.strip()]\n",
    "\n",
    "    data = positive_sentences + negative_sentences\n",
    "    labels = [1]*len(positive_sentences) + [0]*len(negative_sentences)\n",
    "    return data, labels\n",
    "\n",
    "# ---------------------- Part 2: Logistic Regression with Gradient Descent ----------------------\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def _init_(self, lr=0.1, epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        for _ in range(self.epochs):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X) >= 0.5\n",
    "\n",
    "# ---------------------- Part 3: Boyer-Moore Pattern Matching ----------------------\n",
    "\n",
    "def preprocess_patterns(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    return df[0].str.lower().tolist()\n",
    "\n",
    "def bad_char_heuristic(pattern):\n",
    "    bad_char = [-1]*256\n",
    "    for i in range(len(pattern)):\n",
    "        bad_char[ord(pattern[i])] = i\n",
    "    return bad_char\n",
    "\n",
    "def suffixes(pattern):\n",
    "    m = len(pattern)\n",
    "    suff = [0] * m\n",
    "    suff[m-1] = m\n",
    "    g = m - 1\n",
    "    f = 0\n",
    "    for i in range(m-2, -1, -1):\n",
    "        if i > g and suff[i+m-1-f] < i - g:\n",
    "            suff[i] = suff[i+m-1-f]\n",
    "        else:\n",
    "            g = min(g, i)\n",
    "            f = i\n",
    "            while g >= 0 and pattern[g] == pattern[g+m-1-f]:\n",
    "                g -= 1\n",
    "            suff[i] = f - g\n",
    "    return suff\n",
    "\n",
    "def good_suffix_heuristic(pattern):\n",
    "    m = len(pattern)\n",
    "    suff = suffixes(pattern)\n",
    "    good_suffix = [m]*m\n",
    "    j = 0\n",
    "    for i in range(m-1, -1, -1):\n",
    "        if suff[i] == i+1:\n",
    "            for j in range(m-1-i):\n",
    "                if good_suffix[j] == m:\n",
    "                    good_suffix[j] = m-1-i\n",
    "    for i in range(m-1):\n",
    "        good_suffix[m-1-suff[i]] = m-1-i\n",
    "    return good_suffix\n",
    "\n",
    "def boyer_moore_search(text, pattern):\n",
    "    m = len(pattern)\n",
    "    n = len(text)\n",
    "    if m == 0:\n",
    "        return False\n",
    "\n",
    "    bad_char = bad_char_heuristic(pattern)\n",
    "    good_suffix = good_suffix_heuristic(pattern)\n",
    "\n",
    "    s = 0\n",
    "    while s <= n - m:\n",
    "        j = m - 1\n",
    "        while j >= 0 and pattern[j] == text[s + j]:\n",
    "            j -= 1\n",
    "        if j < 0:\n",
    "            return True\n",
    "        else:\n",
    "            s += max(good_suffix[j], j - bad_char[ord(text[s + j])])\n",
    "    return False\n",
    "\n",
    "def find_vulnerable_terms(input_text, terms):\n",
    "    input_text_lower = input_text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', input_text_lower)\n",
    "    found = []\n",
    "    for term in terms:\n",
    "        if (\n",
    "            term in words and\n",
    "            term not in stopword_set and\n",
    "            term not in non_vulnerable_whitelist\n",
    "        ):\n",
    "            found.append(term)\n",
    "    return found\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Part 4: Masking Utility ----------------------\n",
    "\n",
    "def mask_word(word):\n",
    "    if len(word) <= 2:\n",
    "        return \"*\" * len(word)\n",
    "    return word[0] + \"*\" * (len(word)-2) + word[-1]\n",
    "\n",
    "def mask_text(text, words):\n",
    "    for word in words:\n",
    "        pattern = re.compile(r'\\b' + re.escape(word) + r'\\b', re.IGNORECASE)\n",
    "        text = pattern.sub(mask_word(word), text)\n",
    "    return text\n",
    "\n",
    "# ---------------------- Part 5: Runtime Interface ----------------------\n",
    "\n",
    "def run_system():\n",
    "    data, labels = load_training_data()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    blood_terms = preprocess_patterns(\"blood_relations.csv\")\n",
    "\n",
    "    while True:\n",
    "        input_text = input(\"\\nEnter a sentence (or type 'exit'): \").strip()\n",
    "        if input_text.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Step 1: Check with Boyer-Moore\n",
    "        matched = find_vulnerable_terms(input_text, blood_terms)\n",
    "        if matched:\n",
    "            print(f\"Alert: This message contains {len(matched)} vulnerable word(s): {', '.join(matched)}\")\n",
    "            if input(\"Do you want to mask them? (y/n): \").lower() == 'y':\n",
    "                print(\"Masked Output:\", mask_text(input_text, matched))\n",
    "            continue\n",
    "\n",
    "        # Step 2: Use ML if no term matched\n",
    "        vec_input = vectorizer.transform([input_text]).toarray()\n",
    "        pred = clf.predict(vec_input)\n",
    "        if pred[0] == 1:\n",
    "            print(\"Model detected potential vulnerability in the message.\")\n",
    "            words = re.findall(r'\\b\\w+\\b', input_text.lower())\n",
    "            new_terms = [w for w in words if w not in blood_terms and w not in stopword_set and w not in non_vulnerable_whitelist]\n",
    "            for word in new_terms:\n",
    "                choice = input(f\"Do you want to treat '{word}' as a vulnerable term and mask it? (y/n): \").lower()\n",
    "                if choice == 'y':\n",
    "                    print(\"Masked Output:\", mask_text(input_text, [word]))\n",
    "                    blood_terms.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6198a793-2117-4a7e-8840-6b3b1c680d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence (or type 'exit'):  \"Papa and Mumma love spending weekends with Bhai, Behn, Dadaji, Dadiji, Chachu, Chachi, Mamu, Mami, and all the cousins, where we share stories, eat together, laugh loudly, click photos, and create beautiful memories that keep our family bonds strong, making every moment spent with loved ones priceless and full of happiness.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: This message contains 8 vulnerable word(s): bhai, papa, mumma, mamu, chachi, chachu, chachi, love\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to mask them? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Output: \"p**a and m***a l**e spending weekends with b**i, Behn, Dadaji, Dadiji, c****u, c****i, m**u, Mami, and all the cousins, where we share stories, eat together, laugh loudly, click photos, and create beautiful memories that keep our family bonds strong, making every moment spent with loved ones priceless and full of happiness.\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence (or type 'exit'):  \"Papa and Mumma love spending weekends with Bhai, Behen, Dada, Dadi, Chachu, Chachi, Mamu, Maami, and all the cousins, where we share stories, eat together, laugh loudly, click photos, and create beautiful memories that keep our family bonds strong, making every moment spent with loved ones priceless and full of happiness.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: This message contains 12 vulnerable word(s): bhai, behen, papa, mumma, mamu, chachi, maami, dada, dadi, chachu, chachi, love\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to mask them? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Output: \"p**a and m***a l**e spending weekends with b**i, b***n, d**a, d**i, c****u, c****i, m**u, m***i, and all the cousins, where we share stories, eat together, laugh loudly, click photos, and create beautiful memories that keep our family bonds strong, making every moment spent with loved ones priceless and full of happiness.\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a sentence (or type 'exit'):  exit\n"
     ]
    }
   ],
   "source": [
    "run_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0e4e3-ee6e-45ef-92cc-b17f44ff0578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
